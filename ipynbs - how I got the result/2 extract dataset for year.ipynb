{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv to yearcsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getstuff2(year):\n",
    "    written = 0\n",
    "    with open('dbs_orig\\lenta-ru-news.csv', mode='r', encoding='utf-8') as csvfile:\n",
    "        datareader = csv.reader(csvfile)\n",
    "        with open(f'{year}.csv', mode='a+', encoding='utf-8') as output:\n",
    "            #output.write('url,title,text,topic,tags,date')\n",
    "            for num, row in enumerate(datareader):\n",
    "                if row[5].startswith(f'{year}'):\n",
    "                    row[2] = row[2].replace('\\r\\n', ' ').replace('\\n', ' ').replace('\\r', ' ')\n",
    "                    output.write('\\n' + ','.join(row))  # or tsv?\n",
    "                    written = 1\n",
    "                elif not row[0].startswith('http') and written == 1:\n",
    "                    output.write(''.join(row))\n",
    "                elif row[5].startswith(f'{year+1}'):\n",
    "                    written = 0\n",
    "                    print('this is the end')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the end\n"
     ]
    }
   ],
   "source": [
    "getstuff2(2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### с ДБ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vz = sqlite3.connect('vzglyad.db')\n",
    "cur = vz.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "db2csv(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS year, month, day, news\n",
    "\n",
    "def db2csv(year):\n",
    "    cur.execute(f'SELECT year,month,day,text FROM news WHERE year = {year}')\n",
    "    with open (f'vz_{year}.txt', 'w+', encoding='utf-8') as f:\n",
    "        for row in cur.fetchall():\n",
    "            row = list(row)\n",
    "            if not year == 2017:\n",
    "                row[3] = fixcapitals(row[3]).replace('\\n', ' ').strip(' ')\n",
    "            else:\n",
    "                row[3] = row[3].replace('\\n', ' ').replace('\\xa0', ' ').replace('\\r', ' ').replace('\\t', ' ').strip(' ')\n",
    "            if row[1]<10:\n",
    "                row[1] = '0'+str(row[1])\n",
    "            if row[2]<10:\n",
    "                row[2]= '0'+str(row[2])\n",
    "            f.write('\\t'.join([str(it) for it in row]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixcapitals(text):\n",
    "    abbrlist = ['КоАП', 'СПб', 'СПбГУ', 'УЭБиПК', 'ГУЭБиПК', 'ИрГТУ'\n",
    "               'МегаФон', 'МегаФоном', 'МегаФону', 'ШАСизмом', 'ШАСизм',\n",
    "               'ШАСизма', 'ШАСизму', 'АэроСвита', 'ЮТэйр', 'АКИпресс', 'ГАЗель', 'ГАЗели',\n",
    "               'ЕврАз', 'ЛиАЗ', 'ЭкзоМарс', 'КамАЗ', 'МиГ', 'МиГи', 'МиГа', 'КоммерсантЪ',\n",
    "               'ВКонтакте', 'КнААПО', 'СвДП', 'УрГЭУ', 'ЕвроПРО', 'КОИБами',\n",
    "               'ЦНИИмаш', 'ЦНИИмаше', 'ЦНИИмаша', 'ЦНИИмашу', 'РосРАО',\n",
    "               'НТВшники', 'АвтоВАЗ', 'ЦНИИмаш', 'ДнепроГЭС', 'ЧОПовцев', 'ЧОПовцев', 'ЧОПовцы', 'ЧОПовца']\n",
    "    text = text.replace('\\xa0', ' ').replace('\\r', ' ')\n",
    "    text = text.replace('СШАПо', 'США По')\n",
    "    text = text.replace('.', '. ')\n",
    "    text = re.sub('[^\\w0-9’ -]', ' ', text)\n",
    "    tokens = text.split(' ')\n",
    "    for num, token in enumerate(tokens):\n",
    "        if token in abbrlist:\n",
    "            pass\n",
    "        else:\n",
    "            countcapital = 0\n",
    "            \n",
    "            for char in token:\n",
    "                if char.isupper():\n",
    "                    countcapital +=1\n",
    "        \n",
    "            if re.sub('([A-z]+)', ' \\g<1> ', token) != token:\n",
    "                if re.search('[А-ЯЁа-яё]', token):\n",
    "                    tokens[num] = re.sub('([A-z]+)', ' \\g<1> ', token)  \n",
    "            elif all(char.isupper() for char in token):\n",
    "                pass\n",
    "            elif not token.replace('-', '').isalpha():\n",
    "                pass\n",
    "            elif countcapital>=1 and token[0].islower():\n",
    "                if re.findall('([а-яё]+)([А-ЯЁ]+[а-яё]+)', token):\n",
    "                    split = re.findall('([а-яё]+)([А-ЯЁ]+[а-яё]+)', token)[0]\n",
    "                    tokens[num] = ' '.join(split)\n",
    "                elif re.findall('([а-яё]+)([А-ЯЁ]+)', token):\n",
    "                    split = re.findall('([а-яё]+)([А-ЯЁ]+)', token)[0]\n",
    "                    tokens[num] = ' '.join(split)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            elif countcapital>1:\n",
    "                t1 = tokens[num]\n",
    "                if re.findall('([А-ЯЁ][а-яё]+)([А-ЯЁ][а-яё]+)', token):\n",
    "                    split = re.findall('([А-ЯЁ][а-яё]+)([А-ЯЁ][а-яё]+)', token)[0]\n",
    "                    tokens[num] = ' '.join(split)\n",
    "                elif re.findall('([А-ЯЁ]+[а-яё][а-яё]?[А-ЯЁ]+)', token):\n",
    "                    test = re.findall('([А-ЯЁ]+[а-яё][а-яё]?[А-ЯЁ]+)', token)\n",
    "                    if token in test:\n",
    "                        pass\n",
    "                   # else:\n",
    "                    #    print(token)\n",
    "                elif re.findall('([А-ЯЁ][а-яё]+)([А-ЯЁ]+)', token):\n",
    "                    split = re.findall('([А-ЯЁ][а-яё]+)([А-ЯЁ]+)', token)[0]\n",
    "                    tokens[num] = ' '.join(split)\n",
    "    \n",
    "                elif re.findall('([А-ЯЁ]+)([А-ЯЁ]+[а-яё][а-яё][а-яё]+)', token):\n",
    "                    split = re.findall('([А-ЯЁ]+)([А-ЯЁ]+[а-яё][а-яё][а-яё]+)', token)[0]\n",
    "                    tokens[num] = ' '.join(split)\n",
    "                    #if t1!=tokens[num]:\n",
    "                     #   print(tokens[num])\n",
    "                else:  # change nothing\n",
    "                    pass   \n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "  \n",
    "    text = ' '.join(tokens)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatize news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forlemmatization patterns\n",
    "poempattern = re.compile('author>(.+?)<review num', re.MULTILINE|re.DOTALL)\n",
    "tspattern = re.compile('[\\t|\\s]+')\n",
    "linkspattern = re.compile('<a href.+?</a>')\n",
    "# cleanlemmatized\n",
    "notworddigittabhyph = re.compile('[^\\w0-9-\\t]')\n",
    "underscore = re.compile('_')\n",
    "multiplehyphens = re.compile('-+?')\n",
    "righthyphen = re.compile(' -')\n",
    "lefthyphen = re.compile('- ')\n",
    "hyphen = re.compile(' - ')\n",
    "multiplespace = re.compile(' +')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full(inFile, outFile):\n",
    "    with open(inFile, encoding='utf-8') as f:\n",
    "        with open(outFile, 'w+', encoding='utf-8') as t:\n",
    "            for news in islice(f, 1, None):\n",
    "                items = news.split(',')\n",
    "                t.write(items[0] + '$' + ','.join(items[2:-3]) + '\\n')\n",
    "                \n",
    "def cleanlemmatized(lemmatized, outputfname):\n",
    "    with open(lemmatized, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    with open (outputfname, 'w+', encoding='utf-8') as output:\n",
    "        output.write('year\\tmonth\\tday\\tnews')\n",
    "        for line in text.splitlines():\n",
    "            if re.search('}/(\\d{4})/(\\d{2})/(\\d{2})', line):\n",
    "                output.write('\\n')\n",
    "            try:\n",
    "                link, news = line.split('$', 1)\n",
    "                date = re.search('(\\d{4})/(\\d{2})/(\\d{2})', link)\n",
    "                year = date.group(1)\n",
    "                month = date.group(2)\n",
    "                day = date.group(3)\n",
    "                clean = notworddigittabhyph.sub(' ', news)\n",
    "                clean = underscore.sub('', clean)\n",
    "                clean = multiplehyphens.sub('-', clean)\n",
    "                clean = righthyphen.sub(' ', clean)\n",
    "                clean = lefthyphen.sub(' ', clean)\n",
    "                clean = multiplespace.sub(' ', clean)\n",
    "                output.write('\\t'.join([year, month, day, clean.lower().replace('\\t', ' ')]))\n",
    "            except (ValueError, AttributeError):\n",
    "                words = re.findall('{([-\\w0-9]+)?}', line)\n",
    "                output.write(' '.join(words).lower())\n",
    "                \n",
    "def lentatotsv(newscsv, forlem, lemmatized, outputtsv):\n",
    "    full(newscsv, forlem)\n",
    "    os.system(f'mystem.exe {forlem} {lemmatized} -c -l -d')\n",
    "    cleanlemmatized(lemmatized, outputtsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ЗАПУСКАТЬ ЭТО -> ЛЕНТА\n",
    "lentatotsv('2018.csv', '2018forlem.txt', '201lemmatized.txt', 'full2017.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poemclean(lemmatized, outputfname): #checkiflatest\n",
    "    with open(lemmatized, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    with open (outputfname, 'w+', encoding='utf-8') as output:\n",
    "        for line in text.splitlines():\n",
    "            clean = notworddigittabhyph.sub(' ', line)\n",
    "            clean = underscore.sub('', clean)\n",
    "            clean = multiplehyphens.sub('-', clean)\n",
    "            clean = righthyphen.sub(' ', clean)\n",
    "            clean = lefthyphen.sub(' ', clean)\n",
    "            clean = multiplespace.sub(' ', clean)\n",
    "            output.write(clean.strip(' ').lower() + '\\n')\n",
    "            \n",
    "def vztotsv(forlem, lemmatized, outputtsv):\n",
    "    os.system(f'mystem.exe {forlem} {lemmatized} -c -l -d')\n",
    "    poemclean(lemmatized, outputtsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ЗАПУСКАТЬ ЭТО -> ВЗГЛЯД\n",
    "\n",
    "years = [2017]\n",
    "for year in years:\n",
    "    vztotsv(f'vz_{year}.txt', f'vz_{year}_lemmatized.txt', f'{year}vz.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
